---
title: "DUN_Vignette"
author: "Domagoj, Uwe and Niti"
date: "14 Mar 2016"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DUN_vignette}
  %\VignetteEngine{knitr::knitr}
  \usepackage[utf8](inputenc)
---

## Contents  

1.  Cross Validation
    + cv.mab 
    + cv.rf  
    + cv.xgb  
    <br>
2.  F
    + fmeta.knn
    + fmeta.knnprob
    + fmeta.knnprob
    + fmeta.rf
    + fmeta.xgb  
    <br>
3.  Full 
    + full.rf
    + full.xgb  
    <br>
4. Word 

## Descriptions 

```{r}
# text justify

```

This vignette shows a few examples of the main functions within DUN package. You can use R built-in function - *getAnywhere( )*, to see the internal method of any of these functions.  
<br>

**1. Cross Validation**

Cross Validation is a model validation technique used in a predictive model that provides an insight on how the model will generalize to an independent dataset. In cross-validation, you make a fixed number of folds (or partitions) of the data, run the analysis on each fold, and then average the overall error estimate.  

The function starting with *"cv."* performs cross validation on a training set data by first creating indices (using *createFolds()* function from "caret" package) that define which data are held out from the *k*-separate folds. The value of k is set to 5. These indices are then used to create 5 different sets of train and test datasets out of the original training set.  

Depending on which *cv.* function you use, it performs an algorithm in each set of train datasets and creates predictions on the respective test datasets. 

* **cv.mab**: uses *maboost* function from "maboost" package. The number of iteration for the algorithm is set to 250.  
<br>
* **cv.rf**: uses Breiman's random forest algorithm as defined in *randomForest()* function in base R. The number of trees for random forest algorithm is set to 400.    
<br>
* **cv.xgb**: uses *xgboost()* method as define in "xgboost"" library in R. The number of iteration for the algorithm is set to 250.

Finally, it compares the predicted labels and actual labels to calculates the overall accuracy rate for each fold. The final output of these functions is a list containing the overall accuracy rates of each fold and their average along with the Confusion Matrix, and Kappa statistic values.  
<br>
Below we show an example for *cv.mab* function: 

```{r}
# create artificial dataset
# inputsTest <- matrix(rnorm(200), ncol=2)
# inputsTrain <- matrix(rnorm(200), ncol=2)
# classesTrain <- c(rep(0, 50), rep(1, 50))
# cv.mab(train=inputsTrain)
#SHOULD we add that url column cannot be used???????????? ONLY ACCEPTS NUMERIC COLUMNS
# and that column names have to be good????????
```

**2. F**


**3. Full**

