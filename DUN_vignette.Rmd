---
title: "DUN_Vignette"
author: "Domagoj, Uwe and Niti"
date: "14 Mar 2016"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DUN_vignette}
  %\VignetteEngine{knitr::knitr}
  \usepackage[utf8](inputenc)
---

## Contents  

1.  Cross Validation
    + cv.mab 
    + cv.rf  
    + cv.xgb  
    <br>
2.  F
    + fmeta.knn
    + fmeta.knnprob
    + fmeta.knnprob
    + fmeta.rf
    + fmeta.xgb  
    <br>
3.  Full 
    + full.rf
    + full.xgb  
    <br>
4. Word 

## Descriptions 

This vignette shows a few examples of the main functions within DUN package. You can use R built-in function - *getAnywhere( )*, to see the internal method of any of these functions.  
<br>

**1. Cross Validation**

Cross Validation is a model validation technique used in a predictive model that provides an insight on how the model will generalize to an independent dataset. In cross-validation, you make a fixed number of folds (or partitions) of the data, run the analysis on each fold, and then average the overall error estimate. 
<br>

* **cv.mab** : This function performs cross validation on a training set data by first creating indices (using *createFolds()* function from "caret" package) that define which data are held out from the *k*-separate folds. The value of k is set to 5. These indices are then used to create 5 different sets of train and test datasets out of the original training set. It performs a boosting algorithm (using *maboost* function from "maboost" package) in each set of train datasets and creates predictions on the respective test datasets. The number of iteration for the algorithm is set to 250. It then compares the predicted labels and actual labels to calculates the overall accuracy rate for each fold.  
<br>
The final output of the function is a list containing the overall accuracy rates of each fold and their average along with the Confusion Matrix, and Kappa statistic values.  
<br>
Below we show an example for the use of this function. 

```{r}
# create artificial dataset
# inputsTest <- matrix(rnorm(200), ncol=2)
# inputsTrain <- matrix(rnorm(200), ncol=2)
# classesTrain <- c(rep(0, 50), rep(1, 50))
# cv.mab(train=inputsTrain)
#SHOULD we add that url column cannot be used???????????? ONLY ACCEPTS NUMERIC COLUMNS
# and that column names have to be good????????
```

* **cv.rf** : This function performs cross validation on a training set data by using Breiman's random forest algorithm as defined in *randomForest()* function in base R. The number of trees for random forest algorithm is set to 400. As with the previous function, indices are defined to create 5 different sets of train and test datasets out of the original training set. It then calculates the overall accuracy rate for each fold by comparing the predicted labels from random forest and the actual labels.  
<br>
The final output of the function is a list containing the overall accuracy rates of each fold and their average along with the Confusion Matrix, and Kappa statistic values.  
<br>
Below we show an example for the use of this function.

```{r}
# example dataset
# function test

```

* **cv.xgb** : You may choose cv.xgb function if you wish to perform cross validation on your training set data by using *xgboost()* method as define in "xgboost"" library in R. Once again, the train dataset is partitioned into 5 different sets of train and test datasets out of the original training set. The function performs xgboost algorithm on each set of train datasets and creates predictions on the respective test datasets. The number of iteration for the algorithm is set to 250.  
<br>
The final output of the function is a list containing the overall accuracy rates of each fold and their average along with the Confusion Matrix, and Kappa statistic values.  
<br>
Below we show an example for the use of this function.

```{r}
# example dataset
# function test

```

**2. F**


**3. Full**

